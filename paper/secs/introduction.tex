\section{introduction}
The exceptional advances of General-Purpose Graphics Processing Units (GPGPUs) 
in recent years have completely revolutionized the computing paradigm across multiple fields, including cryptocurrency mining \cite{o2014bitcoin,taylor2013bitcoin}, machine learning \cite{coates2013deep,abadi2016tensorflow}, and database technologies \cite{bakkum2010accelerating,kaldewey2012gpu}.
GPUs bring astonishing computational power that is only available from supercomputers in the past. 
The state-of-the-art commercial GPU equipment (GV100) is capable to operate at the speed of 14.8 TFLOPS for single precision arithmetic and 870 GB/s peak memory bandwidth \footnote{Quadro GV100 launched by NVIDIA in March 2018}. 
Meanwhile, as the computing resources of GPUs continue to explode, it makes rooms for more applications to run on a single GPU equipment concurrently. For example, GV100 is built with 5120 CUDA cores and 32GB device memory. Nevertheless, most GPU programs try to occupy as much resources as possible to maximize their performance individually. 
The egoism renders inefficiency when the GPUs run multiple programs simultaneously. Imaging a number of concurrent programs requesting a total memory size larger than the device memory. Interleaving the execution leads to redundant data transfers between CPUs and GPUs through PCIe, which is expensive in nature. 

In this paper, we investigate a fundamental data structure, i.e., \emph{hash table}, which has been implemented on GPUs to accelerate numerous applications, ranging from relational hash join \cite{he2008relational,he2009relational,heimel2013hardware}, data mining \cite{pan2011fast,zhou2010parallel,zhong2014medusa},  key value store \cite{zhang2015mega,hetherington2015memcachedgpu,breslow2016horton} and many others \cite{bowers2010parallel,niessner2013real,pan2010efficient,wu2015gpu}. Existing works \cite{alcantara2009real,zhang2015mega,hong2010mapcg,hetherington2015memcachedgpu,breslow2016horton} focus on the static scenario: they know the data size in advance and allocate a sufficiently large hash table to accommodate all entries. In many cases, the data size varies and static allocation leads to inefficient device memory utilization. To fill this gap, we design an efficient GPU hash table that dynamically adjusts to the size of active entries in the table. The hash table supports efficient memory management by sustaining a guaranteed \emph{filled factor} of the table when the data size changes. There are two major challenges for maintaining a high filled factor for hash tables on GPUs:
\begin{itemize}
	\item It leads to a smaller hash table size with smaller number of keys, hence triggers additional conflicts as multiple threads trying to insert/delete the data, which is particularly expensive under the GPU architecture;
	\item It also means that the table needs to be frequently adjusted to the active data size and we incur costly rehashing to move the entries to a new table when the old table cannot accommodate the adjusted data. 
\end{itemize}

To overcome the aforementioned challenges, we propose a dynamic cuckoo hash table on GPUs. Cuckoo hashing \cite{pagh2004cuckoo} uses a number of hash functions to provide each key with multiple locations instead of one. When a location is occupied, existing key is relocated to make room for the new one. Existing works \cite{alcantara2009real,zhang2015mega,breslow2016horton} have demonstrated great success in speeding up their respective applications by paralleling cuckoo hash on GPUs. However, these works build on inefficient locking mechanisms to handle conflicts, which severely downgrade the performance when high contention occurs. In addition, a complete relocation of the entire hash table is required when the data cannot fit the table. In this work, we offer a number of novel designs for implementing dynamic cuckoo hash tables on GPUs. 

First, we propose a novel voter-based coordination scheme among massive GPUs threads to alleviate the locking costs caused by update contention. 
For each key, we allocate a basket of $b$ locations that properly fit a single cache line of GPUs. Each thread is assigned to an insertion/deletion operation on one key. Instead of immediately acquiring a lock on the key to be updated, a thread will first propose a vote among its warp group and all threads in that same warp collaborate to join the winner thread for its update task on the key in charge. There are two distinguishing advantages for the voter-based coordination: (a) once a conflict is detected on one key, instead of spinning, the warp instantly revotes and switches to another key; (b) a near-optimal load balancing is achieved as a thread will assist other warp-aligned threads, even when the thread finishes its assigned tasks.

Second, we employ the cuckoo hashing scheme with $d$ subtables specified by $d$ hash tables, and introduce a resizing policy to maintain filled factor in a bounded range, while minimizing entries in all subtables relocated at the same time. Insertions and deletions would trigger the hash tables to grow and shrink, if the filled factor falls out of the specified range. To maintain high filled factor while still guarantee resizing efficiency, the proposed policy ensures no subtable can be more than twice as large as any other. Meanwhile, the entries in the hash table are arranged so that each subtable has near-equivalent filled factor.
 In this way, we limit the cost of resizing the hash tables and provide better system availability.
Our theoretical analysis demonstrates the optimality of the scheduling policy in terms of processing updates. 
Empirically, the proposed hash table design is capable to operate efficiently at filled factors exceeding 90\%.

Hereby, we summarize our contributions as follows:
\begin{itemize}
	\item We propose a novel voter-based coordination scheme to alleviate the overhead of handling conflicts for high contention scenarios, when the hash tables get updated.  
	\item We introduce an efficient policy to resize the hash tables and our theoretical analysis has demonstrated the near-optimality of the resizing policy.
	\item We conduct extensive experiments on synthetic and real datasets to showcase the superiority of the proposed approach over several state-of-the-art baselines on GPU hashing. Empirically, our dynamic hash table achieves equivalent or even better efficiency as that of static hash table design with unnecessarily large memory allocation, while still maintaining a filled factors exceeding 90\%. The profiling results have also revealed that our approach efficiently leverages GPU resources and demonstrates near-optimal load balancing. 
\end{itemize}

The remaining part of this paper is organized as follows. Section~\ref{sec:pre} introduces the preliminaries and the background on GPUs, followed by the related work in Section~\ref{sec:rel}. Section~\ref{sec:vot} presents our hash table designs as well as the voter-based coordination scheme.  Section~\ref{sec:dyn} introduces our resizing policy against dynamic updates on hash tables. The experimental results are reported in Section~\ref{sec:exp}. Finally, we conclude the paper in Section~\ref{sec:con}. 